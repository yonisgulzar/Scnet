import os
import math
import time
import json
import random
import argparse
from dataclasses import dataclass

import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras import mixed_precision
from tensorflow.keras.applications import MobileNetV3Large

import tensorflow_addons as tfa

from sklearn.model_selection import GroupKFold, GroupShuffleSplit
from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report
import matplotlib.pyplot as plt
import seaborn as sn

# ----------------------------

tf.compat.v1.disable_eager_execution()  # keep "sample style"
mixed_precision.set_global_policy("mixed_float16")

IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)
IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)

# ----------------------------

import albumentations as A

def build_train_aug(img_size: int):
    return A.Compose([
        A.Rotate(limit=20, p=0.5, border_mode=cv2.BORDER_REFLECT_101),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.Affine(scale=(0.9, 1.1), translate_percent=(0.0, 0.05), p=0.8,
                 fit_output=False, mode=cv2.BORDER_REFLECT_101),
        A.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02, p=0.8),
        A.GaussNoise(var_limit=(5.0, 15.0), p=0.5),
        A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.2),
        A.RandomResizedCrop(img_size, img_size, scale=(0.8, 1.0), ratio=(0.9, 1.1), p=1.0),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

def build_eval_aug(img_size: int):
    return A.Compose([
        A.LongestMaxSize(max_size=img_size),
        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_REFLECT_101),
        A.CenterCrop(img_size, img_size),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

# ----------------------------

@dataclass
class Sample:
    path: str
    label: str
    lesion_id: str

def read_bgr(path: str):
    arr = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_COLOR)
    if arr is None:
        raise FileNotFoundError(path)
    return arr

def make_generator(df: pd.DataFrame, classes: list, img_size: int, aug: A.Compose, batch_size: int, shuffle: bool):
    n = len(df)
    indices = np.arange(n)
    while True:
        if shuffle:
            np.random.shuffle(indices)
        for start in range(0, n, batch_size):
            end = min(start + batch_size, n)
            idx = indices[start:end]
            xs, ys = [], []
            for i in idx:
                row = df.iloc[i]
                img = read_bgr(row['path'])
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = aug(image=img)['image']
                xs.append(img)
                ys.append(classes.index(row['label']))
            x = np.stack(xs).astype(np.float32)
            y = tf.keras.utils.to_categorical(ys, num_classes=len(classes))
            yield x, y

# ----------------------------

def build_scnet(num_classes: int, img_size: int=224) -> tf.keras.Model:
    base = MobileNetV3Large(weights="imagenet", include_top=False,
                            input_shape=(img_size, img_size, 3), include_preprocessing=False)
    x = layers.GlobalAveragePooling2D(name="gap")(base.output)
    x = layers.Dense(512, activation='relu', name="acm_dense_512")(x)
    x = layers.BatchNormalization(name="acm_bn_512")(x)
    x = layers.Dropout(0.5, name="acm_do_512")(x)
    x = layers.Dense(128, activation='relu', name="acm_dense_128")(x)
    x = layers.BatchNormalization(name="acm_bn_128")(x)
    x = layers.Dropout(0.5, name="acm_do_128")(x)
    out = layers.Dense(num_classes, activation='softmax', dtype='float32', name="pred")(x)
    model = models.Model(base.input, out, name="ScNet_MobileNetV3Large")
    return model, base

# ----------------------------

class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, base_lr, total_steps, warmup_steps):
        super().__init__()
        self.base_lr = base_lr
        self.total_steps = float(total_steps)
        self.warmup_steps = float(warmup_steps)
    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        if self.warmup_steps > 0:
            warmup_lr = self.base_lr * (step / tf.maximum(1.0, self.warmup_steps))
        else:
            warmup_lr = self.base_lr
        progress = tf.clip_by_value((step - self.warmup_steps) / tf.maximum(1.0, (self.total_steps - self.warmup_steps)), 0.0, 1.0)
        cosine_lr = 0.5 * self.base_lr * (1.0 + tf.cos(math.pi * progress))
        return tf.where(step < self.warmup_steps, warmup_lr, cosine_lr)

def compute_class_weights(labels: np.ndarray, classes: list):
    counts = pd.Series(labels).value_counts().reindex(classes).fillna(0).astype(int)
    total = counts.sum()
    weights = {i: (total / (len(classes) * c)) for i, c in enumerate(counts)}
    return weights

class MacroF1Callback(tf.keras.callbacks.Callback):
    def __init__(self, val_data, steps):
        super().__init__()
        self.val_data = val_data
        self.steps = steps
        self.best = -np.inf
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        y_true, y_pred = [], []
        for _ in range(self.steps):
            x, y = next(self.val_data)
            p = self.model.predict(x, verbose=0)
            y_true.append(np.argmax(y, axis=1))
            y_pred.append(np.argmax(p, axis=1))
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        f1 = f1_score(y_true, y_pred, average='macro')
        logs['val_macro_f1'] = f1
        print(f"\n[MacroF1] val_macro_f1={f1:.4f}")
        if f1 > self.best:
            self.best = f1

# ------------------------

def train_dev_fold(train_df, val_df, classes, img_size, batch_size, epochs, warmup_epochs, out_dir):
    os.makedirs(out_dir, exist_ok=True)

    train_gen = make_generator(train_df, classes, img_size, build_train_aug(img_size), batch_size, shuffle=True)
    val_gen   = make_generator(val_df,   classes, img_size, build_eval_aug(img_size),   batch_size, shuffle=False)

    steps_per_epoch = math.ceil(len(train_df) / batch_size)
    val_steps = math.ceil(len(val_df) / batch_size)

    model, base = build_scnet(len(classes), img_size)
    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05)

    # class weights
    cw = compute_class_weights(train_df['label'].values, classes)

    # TensorBoard
    logdir = os.path.join(out_dir, "tb_logs")
    tb_cb  = tf.keras.callbacks.TensorBoard(log_dir=logdir, write_graph=False, write_images=False)


    for l in base.layers:
        l.trainable = False

    warm_total_steps = steps_per_epoch * warmup_epochs
    warm_lr_head = WarmUpCosine(base_lr=5e-4, total_steps=warm_total_steps, warmup_steps=5 * steps_per_epoch)
    opt_head = tfa.optimizers.AdamW(learning_rate=warm_lr_head, weight_decay=1e-4, global_clipnorm=1.0)

    model.compile(optimizer=opt_head, loss=loss_fn, metrics=['accuracy'])

    macro_cb = MacroF1Callback(val_data=val_gen, steps=val_steps)
    ckpt_warm = os.path.join(out_dir, 'best_warm.h5')
    ckpt_cb1 = tf.keras.callbacks.ModelCheckpoint(ckpt_warm, monitor='val_macro_f1', mode='max',
                                                 save_best_only=True, save_weights_only=False)
    es_cb1 = tf.keras.callbacks.EarlyStopping(monitor='val_macro_f1', mode='max', patience=10, restore_best_weights=True)

    model.fit(train_gen,
              steps_per_epoch=steps_per_epoch,
              validation_data=val_gen,
              validation_steps=val_steps,
              epochs=warmup_epochs,
              class_weight=cw,
              callbacks=[macro_cb, ckpt_cb1, es_cb1, tb_cb],
              verbose=1)


    for l in base.layers:
        l.trainable = True

    ft_total_steps = steps_per_epoch * (epochs - warmup_epochs)
    lr_backbone = WarmUpCosine(base_lr=1e-4, total_steps=ft_total_steps, warmup_steps=5 * steps_per_epoch)
    lr_head     = WarmUpCosine(base_lr=5e-4, total_steps=ft_total_steps, warmup_steps=5 * steps_per_epoch)

    
    back_vars = [v for v in model.trainable_variables if 'mobilenetv3' in v.name.lower()]
    head_vars = [v for v in model.trainable_variables if ('acm_' in v.name.lower()) or ('pred' in v.name.lower())]

    opt_back = tfa.optimizers.AdamW(learning_rate=lr_backbone, weight_decay=1e-4)
    opt_head = tfa.optimizers.AdamW(learning_rate=lr_head,     weight_decay=1e-4)

    multi_opt = tfa.optimizers.MultiOptimizer([
        (opt_back, back_vars),
        (opt_head, head_vars),
    ])

    model.compile(optimizer=multi_opt, loss=loss_fn, metrics=['accuracy'])

    macro_cb2 = MacroF1Callback(val_data=val_gen, steps=val_steps)
    ckpt_ft = os.path.join(out_dir, 'best_ft.h5')
    ckpt_cb2 = tf.keras.callbacks.ModelCheckpoint(ckpt_ft, monitor='val_macro_f1', mode='max',
                                                 save_best_only=True, save_weights_only=False)
    es_cb2 = tf.keras.callbacks.EarlyStopping(monitor='val_macro_f1', mode='max', patience=10, restore_best_weights=True)

    model.fit(train_gen,
              steps_per_epoch=steps_per_epoch,
              validation_data=val_gen,
              validation_steps=val_steps,
              epochs=(epochs - warmup_epochs),
              class_weight=cw,
              callbacks=[macro_cb2, ckpt_cb2, es_cb2, tb_cb],
              verbose=1)

    return model

# ------------------------

def eval_on_df(model, df: pd.DataFrame, classes, img_size, batch_size):
    gen = make_generator(df, classes, img_size, build_eval_aug(img_size), batch_size, shuffle=False)
    steps = math.ceil(len(df) / batch_size)
    y_true, y_pred = [], []
    for _ in range(steps):
        x, y = next(gen)
        p = model.predict(x, verbose=0)
        y_true.append(np.argmax(y, axis=1))
        y_pred.append(np.argmax(p, axis=1))
    y_true = np.concatenate(y_true)
    y_pred = np.concatenate(y_pred)
    P, R, F1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)
    return P, R, F1, (y_true, y_pred)

def plot_confusion(y_true, y_pred, classes, tag):
    cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=len(classes)).numpy()
    df_cm = pd.DataFrame(cm, index=classes, columns=classes)
    os.makedirs('plots', exist_ok=True)
    plt.figure(figsize=(8,5))
    plt.title(f"Confusion Matrix — {tag}")
    s = sn.heatmap(df_cm, annot=True, cmap="BuPu", annot_kws={"size":12}, fmt='d')
    s.set(xlabel='Predicted', ylabel='True')
    plt.savefig(f'plots/conf_{tag}.png')

# ------------------------

def count_params_by_block(model: tf.keras.Model):
    """Return (backbone_params, head_params) based on variable names."""
    back, head = 0, 0
    for v in model.trainable_variables:
        n = int(np.prod(v.shape.as_list()))
        name = v.name.lower()
        if 'mobilenetv3' in name:
            back += n
        else:
            head += n
    return back, head

def measure_latency(model: tf.keras.Model, img_size=224, runs=50, warmup=10):
    """Average bs=1 latency over N runs (GPU), with warmup."""
    x = np.random.randn(1, img_size, img_size, 3).astype(np.float32)
    for _ in range(warmup):
        _ = model.predict(x, verbose=0)
    t0 = time.time()
    for _ in range(runs):
        _ = model.predict(x, verbose=0)
    t1 = time.time()
    return 1000.0 * (t1 - t0) / runs  # ms

# ------------------------

def model_stats(model: tf.keras.Model, img_size: int):
    params = model.count_params()
    # FLOPs (best-effort)
    flops = None
    try:
        concrete = tf.function(lambda x: model(x)).get_concrete_function(tf.TensorSpec([1, img_size, img_size, 3], tf.float32))
        frozen = tf.graph_util.convert_variables_to_constants_v2(concrete)
        graph = frozen.graph
        run_meta = tf.compat.v1.RunMetadata()
        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
        prof = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd='op', options=opts)
        flops = prof.total_float_ops
    except Exception:
        pass
    # Stable latency (GPU)
    try:
        latency_ms = measure_latency(model, img_size=img_size, runs=50, warmup=10)
    except Exception:
        latency_ms = None

    # Print breakdown here (keeps external call sites unchanged)
    back_params, head_params = count_params_by_block(model)
    print(f"Params breakdown — backbone: {back_params:,}, ACM/head: {head_params:,}, total: {params:,}")

    return params, flops, latency_ms

# ------------------------

def main(args):
    # --- Local environment evidence (unchanged) ---
    import socket, platform, subprocess, shutil
    env = {
        "timestamp": __import__("datetime").datetime.now().isoformat(),
        "cwd": os.getcwd(),
        "host": socket.gethostname(),
        "platform": platform.platform(),
        "python": platform.python_version(),
        "tf_version": tf.__version__,
        "tfa_version": getattr(tfa, "__version__", "unknown"),
        "mixed_precision": str(tf.keras.mixed_precision.global_policy()),
        "gpus": [d.name for d in tf.config.list_physical_devices("GPU")],
    }
    try:
        build = tf.sysconfig.get_build_info()
        env["cuda_version"] = build.get("cuda_version")
        env["cudnn_version"] = build.get("cudnn_version")
    except Exception:
        pass
    try:
        env["pip_freeze"] = subprocess.check_output(
            [shutil.which("pip") or "pip", "freeze"], text=True, stderr=subprocess.STDOUT
        ).splitlines()
    except Exception as e:
        env["pip_freeze_error"] = str(e)
    try:
        env["nvidia_smi"] = subprocess.check_output(["nvidia-smi"], text=True)
    except Exception:
        env["nvidia_smi"] = "n/a"
    try:
        env["git_commit"] = subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        env["git_commit"] = "n/a"

    os.makedirs(args.out, exist_ok=True)
    with open(os.path.join(args.out, "env_runtime.json"), "w", encoding="utf-8") as f:
        json.dump(env, f, indent=2)
    print("[ENV] wrote", os.path.join(args.out, "env_runtime.json"))
    # --- End local evidence ---

    df = pd.read_csv(args.csv)
    assert {'path','label','lesion_id'}.issubset(df.columns), "CSV must have: path,label,lesion_id"
    classes = sorted(df['label'].unique().tolist())

    splits_root = os.path.join(args.out, "splits")
    cv_root = os.path.join(splits_root, "cv5")
    os.makedirs(cv_root, exist_ok=True)


    if args.use_saved_splits and \
       os.path.exists(os.path.join(splits_root, "dev.csv")) and \
       os.path.exists(os.path.join(splits_root, "test.csv")):
        print("[SPLITS] Using saved splits from", splits_root)
        dev_df  = pd.read_csv(os.path.join(splits_root, "dev.csv"))
        test_df = pd.read_csv(os.path.join(splits_root, "test.csv"))

        # load fold CSVs if present
        folds = []
        all_found = True
        for k in range(args.folds):
            tr_p = os.path.join(cv_root, f"fold_{k}_train.csv")
            va_p = os.path.join(cv_root, f"fold_{k}_val.csv")
            if not (os.path.exists(tr_p) and os.path.exists(va_p)):
                all_found = False
                break
            folds.append((pd.read_csv(tr_p).reset_index(drop=True),
                          pd.read_csv(va_p).reset_index(drop=True)))
        if not all_found:
            print("[SPLITS] CV fold CSVs not found; will regenerate folds from dev_df.")
            folds = None
    else:
        # Generate new lesion-wise splits and SAVE them
        print("[SPLITS] Generating new lesion-wise splits (saved to", splits_root, ")")
        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=args.seed)
        dev_idx, test_idx = next(gss.split(df, groups=df['lesion_id']))
        dev_df  = df.iloc[dev_idx].reset_index(drop=True)
        test_df = df.iloc[test_idx].reset_index(drop=True)
        dev_df.to_csv(os.path.join(splits_root, "dev.csv"), index=False)
        test_df.to_csv(os.path.join(splits_root, "test.csv"), index=False)
        folds = None

    # Prepare 5-fold CV (either load or create and save)
    if folds is None:
        gkf = GroupKFold(n_splits=args.folds)
        folds = []
        for k, (tr_idx, va_idx) in enumerate(gkf.split(dev_df, groups=dev_df['lesion_id'])):
            tr_df = dev_df.iloc[tr_idx].reset_index(drop=True)
            va_df = dev_df.iloc[va_idx].reset_index(drop=True)
            folds.append((tr_df, va_df))
            tr_df.to_csv(os.path.join(cv_root, f"fold_{k}_train.csv"), index=False)
            va_df.to_csv(os.path.join(cv_root, f"fold_{k}_val.csv"), index=False)
        print("[SPLITS] Saved CV folds to", cv_root)

  
    fold_metrics = []
    last_model = None
    for fold, (train_df, val_df) in enumerate(folds):
        print(f"\n========== Fold {fold+1}/{args.folds} ==========")
        out_dir = os.path.join(args.out, f"fold_{fold+1}")
        model = train_dev_fold(
            train_df=train_df,
            val_df=val_df,
            classes=classes,
            img_size=args.image_size,
            batch_size=args.batch_size,
            epochs=args.epochs,
            warmup_epochs=20,
            out_dir=out_dir,
        )
        P, R, F1, (yt, yp) = eval_on_df(model, val_df, classes, args.image_size, args.batch_size)
        print(f"Fold {fold+1} — Val Macro: P={P:.4f} R={R:.4f} F1={F1:.4f}")
        fold_metrics.append((P, R, F1))
        last_model = model

    cv_prec = float(np.mean([m[0] for m in fold_metrics]))
    cv_rec  = float(np.mean([m[1] for m in fold_metrics]))
    cv_f1   = float(np.mean([m[2] for m in fold_metrics]))
    print(f"\n==== CV mean over {args.folds} folds — Macro: P={cv_prec:.4f} R={cv_rec:.4f} F1={cv_f1:.4f}")


    params, flops, latency = model_stats(last_model, args.image_size)
    print(f"Params (total): {params:,}")
    if flops is not None:
        print(f"Approx FLOPs: {flops:,}")
    else:
        print("FLOPs: (not available on this TF build)")
    if latency is not None:
        print(f"GPU latency (bs=1, mean over 50 runs): {latency:.2f} ms")

  
    P, R, F1, (yt, yp) = eval_on_df(last_model, test_df, classes, args.image_size, args.batch_size)
    print(f"\n==== HOLD-OUT TEST — Macro: P={P:.4f} R={R:.4f} F1={F1:.4f}")
    plot_confusion(yt, yp, classes, tag="test")

if __name__ == '__main__':
    ap = argparse.ArgumentParser()
    ap.add_argument('--csv', required=True, help='CSV with columns: path,label,lesion_id')
    ap.add_argument('--image-size', type=int, default=224)
    ap.add_argument('--batch-size', type=int, default=32)
    ap.add_argument('--epochs', type=int, default=100)
    ap.add_argument('--folds', type=int, default=5)
    ap.add_argument('--seed', type=int, default=42)
    ap.add_argument('--out', default='./runs_scnet')
    # NEW: reuse previously saved splits so the reviewer can reproduce exactly
    ap.add_argument('--use-saved-splits', action='store_true',
                    help='Load lesion-wise dev/test and CV fold CSVs from OUT/splits if present.')
    args = ap.parse_args()

    os.makedirs(args.out, exist_ok=True)

    # Deterministic-ish
    os.environ['PYTHONHASHSEED'] = str(args.seed)
    random.seed(args.seed)
    np.random.seed(args.seed)
    tf.random.set_seed(args.seed)

    main(args)
